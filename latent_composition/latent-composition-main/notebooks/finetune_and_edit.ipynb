{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils import show, renormalize, masking\n",
    "from utils import util, imutil, pbar, losses, inversions\n",
    "from networks import networks\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nets():\n",
    "    \n",
    "    # bonus stylegan encoder trained on real images + identity loss\n",
    "    # nets = networks.define_nets('stylegan', 'ffhq', ckpt_path='pretrained_models/sgan_encoders/ffhq_reals_RGBM/netE_epoch_best.pth')\n",
    "    \n",
    "    # stylegan trained on gsamples + identity loss\n",
    "    nets = networks.define_nets('stylegan', 'ffhq')\n",
    "    return nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = 'img/torralba_cropped.png'\n",
    "\n",
    "outdim=1024 # for faces\n",
    "# outdim = 256 # for churches\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(outdim),\n",
    "                transforms.CenterCrop(outdim),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])    \n",
    "source_im = transform(Image.open(im_path).convert('RGB'))[None].cuda()\n",
    "show(['Source Image', renormalize.as_image(source_im[0]).resize((256, 256), Image.LANCZOS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reload nets each time after finetuning\n",
    "nets = load_nets()\n",
    "outdim = nets.setting['outdim']\n",
    "\n",
    "with torch.no_grad():\n",
    "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
    "    out = nets.invert(source_im, mask)\n",
    "    # encoded = nets.encode(source_im, mask)\n",
    "    # out = nets.decode(encoded)\n",
    "    show(['Inverted Image', renormalize.as_image(out[0]).resize((256, 256), Image.LANCZOS)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune the encoder towards the real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.psp import id_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# need to reload nets each time after finetuning\n",
    "nets = load_nets()\n",
    "outdim = nets.setting['outdim']\n",
    "\n",
    "with torch.no_grad():\n",
    "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
    "    initial_inversion = nets.invert(source_im, mask)\n",
    "\n",
    "batch_size = 1\n",
    "lambda_mse = 1.0\n",
    "lambda_lpips = 1.0\n",
    "lambda_z = 0. # set lambda_z to 10.0 to optimize the latent first. (optional)\n",
    "lambda_id = 0.1\n",
    "\n",
    "# do optional latent optimization\n",
    "if lambda_z > 0.:\n",
    "    checkpoint_dict, opt_losses = inversions.invert_lbfgs(nets, source_im, num_steps=30)\n",
    "    opt_ws = checkpoint_dict['current_z'].detach().clone().repeat(batch_size, 1, 1)\n",
    "    # reenable grad after LBFGS\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "netG = nets.generator.eval()\n",
    "netE = nets.encoder.eval()\n",
    "util.set_requires_grad(False, netG)\n",
    "util.set_requires_grad(True, netE)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "l1_loss = torch.nn.L1Loss()\n",
    "perceptual_loss = losses.LPIPS_Loss().cuda().eval()\n",
    "identity_loss = id_loss.IDLoss().cuda().eval()\n",
    "util.set_requires_grad(False, identity_loss)\n",
    "util.set_requires_grad(False, perceptual_loss)\n",
    "\n",
    "optimizer = torch.optim.Adam(netE.parameters(), lr=0.00005, betas=(0.5, 0.999))\n",
    "\n",
    "target = source_im.repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "reshape = torch.nn.AdaptiveAvgPool2d((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = dict(z=[], mse=[], lpips=[], id=[], sim_improvement=[])\n",
    "\n",
    "# 30-50 steps is about enough\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for i in pbar(range(30)):\n",
    "    optimizer.zero_grad()\n",
    "    mask_data = [masking.mask_upsample(source_im) for _ in range(batch_size)]\n",
    "    hints = torch.cat([m[0] for m in mask_data])\n",
    "    masks = torch.cat([m[1] for m in mask_data])\n",
    "    \n",
    "    encoded = netE(torch.cat([hints, masks], dim=1))\n",
    "    regenerated = netG(encoded)\n",
    "    if lambda_z > 0.:\n",
    "        loss_z = mse_loss(encoded, opt_ws)\n",
    "    else:\n",
    "        loss_z = torch.Tensor((0.,)).cuda()\n",
    "    loss_mse = mse_loss(regenerated, target)\n",
    "    loss_perceptual = perceptual_loss.forward(\n",
    "        reshape(regenerated), reshape(target)).mean()\n",
    "    loss_id, sim_improvement, id_logs = identity_loss(reshape(regenerated), reshape(target), reshape(target))\n",
    "    loss = (lambda_z * loss_z + lambda_mse * loss_mse\n",
    "            + lambda_lpips * loss_perceptual + lambda_id * loss_id)\n",
    "    # loss.backward(retain_graph=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_losses['z'].append(loss_z.item())\n",
    "    all_losses['mse'].append(loss_mse.item())\n",
    "    all_losses['lpips'].append(loss_perceptual.item())\n",
    "    all_losses['id'].append(loss_id.item())\n",
    "    all_losses['sim_improvement'].append(sim_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,4, figsize=(16, 3))\n",
    "ax[0].plot(all_losses['z'])\n",
    "ax[0].set_title('Z loss')\n",
    "ax[1].plot(all_losses['mse'])\n",
    "ax[1].set_title('MSE loss')\n",
    "ax[2].plot(all_losses['lpips'])\n",
    "ax[2].set_title('LPIPS loss')\n",
    "ax[3].plot(all_losses['id'])\n",
    "ax[3].set_title('ID loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show.a(['Initial Inversion', renormalize.as_image(initial_inversion[0]).resize((256, 256), Image.LANCZOS)])\n",
    "if lambda_z > 0.:\n",
    "    show.a(['optimized w', renormalize.as_image(checkpoint_dict['current_x'][0]).resize((256, 256), Image.LANCZOS)])\n",
    "show.flush()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    hints = source_im\n",
    "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
    "    \n",
    "    # hints, mask = masking.mask_upsample(source_im, threshold=0.5) \n",
    "    # mask = mask+0.5\n",
    "    \n",
    "    encoded = nets.encode(hints, mask)\n",
    "    out = nets.decode(encoded)\n",
    "    show.a(['hints Image', renormalize.as_image(hints[0]).resize((256, 256), Image.LANCZOS)])\n",
    "    show.a(['Inverted Image', renormalize.as_image(out[0]).resize((256, 256), Image.LANCZOS)])\n",
    "    show.flush()\n",
    "    \n",
    "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
    "    mask[:, :, 100:-100, 100:-100] = 0.\n",
    "    hints = source_im*mask\n",
    "    \n",
    "    encoded = nets.encode(hints, mask)\n",
    "    out = nets.decode(encoded)\n",
    "    show.a(['Hints Image', renormalize.as_image(hints[0]).resize((256, 256), Image.LANCZOS)])\n",
    "    show.a(['Inverted Hints', renormalize.as_image(out[0]).resize((256, 256), Image.LANCZOS)])\n",
    "    show.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interactive mixing\n",
    "Draw your mouse on the image panels. The network input will show in the second to last panel, and the network output in the last panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collage_paths = [\n",
    "    im_path,\n",
    "    'img/efros_cropped.png',\n",
    "    'img/phil_cropped.png',\n",
    "    'img/biden_cropped.png'\n",
    "]\n",
    "num_components = len(collage_paths)\n",
    "collage_ims = torch.cat([transform(Image.open(p).convert('RGB'))[None].cuda() for p in collage_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import paintwidget, labwidget\n",
    "\n",
    "def make_callback(painter):\n",
    "    def probe_changed(c):\n",
    "        global composite\n",
    "        global mask_composite\n",
    "        p = painter\n",
    "        if p.mask:\n",
    "            mask = renormalize.from_url(p.mask, target='pt', size=(outdim, outdim)).cuda()[None]\n",
    "        else:\n",
    "            mask = torch.zeros_like(sample)[None]\n",
    "        with torch.no_grad():\n",
    "            mask = mask[:, [0], :, :].cuda()\n",
    "            mask_composite += mask\n",
    "            sample = renormalize.from_url(p.image, size=(outdim, outdim)).cuda()[None]\n",
    "            \n",
    "            composite = sample * mask + composite * (1-mask)\n",
    "            mask_composite = torch.clamp(mask_composite, 0., 1.)\n",
    "            out = nets.invert(composite, mask_composite)\n",
    "        img_url = renormalize.as_url(composite[0], size=256)\n",
    "        img_html = '<img src=\"%s\"/>'% img_url\n",
    "        collage_div.innerHTML = img_html   \n",
    "        img_url = renormalize.as_url(out[0], size=256)\n",
    "        img_html = '<img src=\"%s\"/>'% img_url\n",
    "        encoded_div.innerHTML = img_html\n",
    "    return probe_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = renormalize.as_url(torch.zeros(3, outdim, outdim), size=256)\n",
    "img_html = '<img src=\"%s\"/>'%img_url\n",
    "encoded_div = labwidget.Div(img_html)\n",
    "collage_div = labwidget.Div(img_html)\n",
    "\n",
    "painters = []\n",
    "\n",
    "composite = torch.zeros(1, 3, outdim, outdim).cuda()\n",
    "mask_composite = torch.zeros_like(composite)[:, [0], :, :]\n",
    "\n",
    "for i in range(num_components):\n",
    "    src_painter = paintwidget.PaintWidget(oneshot=False, width=256, height=256, \n",
    "                                      brushsize=20, save_sequence=False, track_move=True) # , on_move=True)\n",
    "    src_painter.image = renormalize.as_url(collage_ims[i], size=256)\n",
    "    painters.append(src_painter)\n",
    "    callback = make_callback(src_painter)\n",
    "    src_painter.on('mask', callback)\n",
    "    show.a([src_painter], cols=3)\n",
    "\n",
    "show.a([collage_div], cols=3)\n",
    "show.a([encoded_div], cols=3)\n",
    "show.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_drawing():\n",
    "    for i, p in enumerate(painters):\n",
    "        if p.mask:\n",
    "            mask = renormalize.from_url(p.mask, target='pt', size=(outdim, outdim)).cuda()[None]\n",
    "        else:\n",
    "            mask = torch.zeros(1, 3, outdim, outdim).cuda()\n",
    "        mask = mask[:, [0], :, :].cuda()\n",
    "        sample = renormalize.from_url(p.image, size=(outdim, outdim)).cuda()[None]\n",
    "        part = sample * mask\n",
    "        im_pil = imutil.draw_masked_image(sample, mask, size=256)[1]\n",
    "        # im_pil.save(os.path.join(save_path, 'part%d.png' % i))\n",
    "        show.a(['part %d' % i, im_pil.resize((200, 200), Image.ANTIALIAS)], cols=3)\n",
    "    with torch.no_grad():\n",
    "        out = nets.invert(composite, mask_composite)\n",
    "    composite_pil = renormalize.as_image(out[0])\n",
    "    input_np = np.array(renormalize.as_image(composite[0]))\n",
    "    mask_np = np.stack([np.array(mask_composite.cpu()[0][0])] * 3, axis=2)\n",
    "    input_np[mask_np == 0] = 200 # lighten the unfilled region\n",
    "    input_pil = Image.fromarray(input_np)\n",
    "    # input_pil = renormalize.as_image(composite[0])\n",
    "    # composite_pil.save(os.path.join(save_path, 'composite.png'))\n",
    "    show.a(['input', input_pil.resize((200, 200), Image.ANTIALIAS)])\n",
    "    show.a(['composite', composite_pil.resize((200, 200), Image.ANTIALIAS)])\n",
    "    show.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_drawing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latent-composition",
   "language": "python",
   "name": "latent-composition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}